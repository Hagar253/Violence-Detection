# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cpyr7ypdsBHVnlT8k3mVy89IL5iE107p
"""

!pip install tensorflow

import os
import cv2
import numpy as np
import pandas as pd
import tensorflow as tf
import random
import math
import datetime as dt
import matplotlib.pyplot as plt

# dataset = os.listdir('/content/drive/MyDrive/vio_data/Real Life Violence Dataset')
# label = os.listdir('/content/drive/MyDrive/vio_data/Real Life Violence Dataset')
# print(label)

from inspect import FrameInfo
#create figure with sepecific size
plt.figure(figsize = (40,40))
# to get the labels of data by folder name of directory
class_names = os.listdir('/content/drive/MyDrive/vio_data/Real Life Violence Dataset')
# to get random sample from the classes each time we run
# it takes the range length of class name and length of the sample
random_sample = random.sample(range(len(class_names)),2)
# iterating through random videos we get , counter for the index of the video that starts with 1 and video index is for the index of random videos we select
for counter , video_index in enumerate(random_sample,1):
  # retrive the class name from video index that consist of 0 for the first class and 1 for the second class
  select_class = class_names[video_index]
  # retrive a list of names of all videos in the directory folder for each class
  all_videos = os.listdir(f'/content/drive/MyDrive/vio_data/Real Life Violence Dataset/{select_class}')
  print(all_videos)
  # select a video from the list of videos we have got for each class
  select_data = random.choice(all_videos)
  # to read the video file that has been selected randomly
  video_reader = cv2.VideoCapture(f'/content/drive/MyDrive/vio_data/Real Life Violence Dataset/{select_class}/{select_data}')
  # to read the first frame from the video
  _,frame = video_reader.read()
  # after reading the first frame , stop reading
  video_reader.release()
  # convert the frame from bgr to rgb to be able to display it
  rgb_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)
  # to write name of the class as a text on the image
  cv2.putText(rgb_frame , select_class , (10,30),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)
  # to put 2 classes beside each other and to show the image
  plt.subplot(5,4,counter)
  plt.imshow(rgb_frame)
  plt.axis('off')

# specify the size of the frames
# if size increase the complexity increase and accuracy increase
#64*64 is enough to know what happened in the frames
image_h = 64
image_w = 64
# sequence of the frame that he will take when he slice the video
sequence_frame = 10
# # dataset directory
# dataset = str(os.listdir('/content/drive/MyDrive/vio_data/Real Life Violence Dataset'))
#classes name
classes = ['Violence', 'NonViolence']

# this function will extract frames from the video
def frames_from_videos(video_path):
  # list to store the frames that have been taken from video
  frames = []
  # to read the video file that has been selected randomly
  video_reader = cv2.VideoCapture(video_path)
  # to get the total number of frames in the video
  #cv2.CAP_PROP_FRAME_COUNT it calculates total number of frames in video
  total_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
  ''' due to we have a certain number of sequence frames we have to get
       we will skip some frames through interval of time
       so we will divide the total number of frames that we have got from video
       from the sequence frame that i want to get it we generate a number that
       consider as the time interval where next frame we be taken

       '''
  skip_frames = max(int(total_frames/sequence_frame),1)

  # iterate through the video frames
  for counter in range(sequence_frame):
    # know the current postion of the frame
    # cv2.CAP_PROP_POS_FRAMES to get the postion of the frame you take
    ''' counter * skip frames is to read the frames at certain time not from the beginning
        because we want to read certain frames not all frames that we have been selected in
        skip frames then we will read video from this point
    '''
    video_reader.set(cv2.CAP_PROP_POS_FRAMES , counter * skip_frames)

    # reading the frames from the video in certain point of time
    # sec is boolean value if it reads the frame or not
    # frame is the real frame
    sec , frame = video_reader.read()
    # to check if the frames has been sucessfuly read or not
    if not sec:
      break

    # resize the frame to fixed height and width
    frame_resize = cv2.resize(frame ,(image_h,image_w))
    # normalize the resized frame to be from 0 to 1 to help in training
    frame_normalization = frame_resize/255
    # append the frame into the frame list
    frames.append(frame_normalization)

  # video release after taking frames
  video_reader.release()
  # retrun frame list
  return frames

# it will takes the frames that has been extracted and select it to classes and create normal dataset
def new_dataset():
  # to store extracted frames
  extracted_frames = []
  # to store labels
  labels = []
  # to store file path
  file_path = []

  # iterate through all the classes we have
  for class_index , class_name in enumerate (classes):
    # print hte name of the class whose frames has been extracted
    print(f'Extracted frames from class:{class_name}')
    # list of videos in specific directory
    list_video = os.listdir(os.path.join('/content/drive/MyDrive/vio_data/Real Life Violence Dataset',class_name))
    #iterate through the videos in the directory
    for vid in list_video:
      # to get the path of video
      video_path = os.path.join('/content/drive/MyDrive/vio_data/Real Life Violence Dataset' , class_name , vid)
      # extract frames from function frames from video
      # we will get 10 frames from each video in each class
      frames = frames_from_videos(video_path)

      # check if the number of frames the same as our sequence length
      if len(frames) == sequence_frame:
        # if true then append the data of video in lists
        extracted_frames.append(frames)
        labels.append(class_index)

        file_path.append(video_path)
# converting list to numpy array
  extracted_frames = np.asarray(extracted_frames)
  labels = np.array(labels)

  # return frames with labels and path
  return extracted_frames, labels , file_path

# create dataset
features, cla, path = new_dataset()

# one hot encode the categorical column
from tensorflow.keras.utils import to_categorical
encoding_class_labels = to_categorical(cla)

# splitting data into training and testing
from sklearn.model_selection import train_test_split
seed_constant = 27
x_train , x_test , y_train , y_test = train_test_split(features,encoding_class_labels,test_size = 0.20 , shuffle = True ,random_state = seed_constant )

print(x_train)